# Fourth Assignment – Deep RL Benchmarks

## Introduction

Reinforcement learning research relies on reproducible benchmarks such as Gymnasium Classic Control and MuJoCo
continuous-control tasks【turn0search3】【turn0search8】. Stable-Baselines3 (SB3) provides vetted PyTorch implementations of
state-of-the-art algorithms【turn0search6】【turn0search11】, while RL-Baselines3-Zoo offers tuned hyper-parameters and
pretrained scores for comparison【turn0search2】【turn0search7】. This assignment evaluates three agents—A2C, PPO and
TD3—across two discrete-action and one continuous-action environments on a macOS workstation.

## Method

| Component        | File                             | Purpose                                                        |
|------------------|----------------------------------|----------------------------------------------------------------|
| Benchmark list   | `fourthAssignment/benchmarks.py` | Central registry of environment IDs and algorithm names        |
| Training loop    | `fourthAssignment/train.py`      | Iterates over benchmarks, logs to TensorBoard and saves models |
| Evaluation loop  | `fourthAssignment/evaluate.py`   | Runs 20 rollouts per model and prints mean episodic return     |
| Saved weights    | `models/…`                       | Auto-generated by `train.py`                                   |
| TensorBoard logs | `runs/…`                         | Auto-generated by SB3                                          |

Hardware & software

* MacBook Pro M4, 16 GB RAM, macOS 15
* Python 3.12, PyTorch 2.3, SB3 2.4.0
* Gymnasium 1.5.0, MuJoCo 2.3.8 (`MUJOCO_GL=metal`)

Training budget

* Classic tasks: 0.5 M steps
* MuJoCo task: 1 M steps

## Results

| Environment    | A2C                  | PPO                  | TD3                  |
|----------------|----------------------|----------------------|----------------------|
| CartPole-v1    | <!-- mean return --> | <!-- mean return --> | n/a                  |
| LunarLander-v3 | <!-- mean return --> | <!-- mean return --> | n/a                  |
| Hopper-v4      | n/a                  | <!-- mean return --> | <!-- mean return --> |

## Discussion

PPO outperformed A2C on both discrete benchmarks, likely due to advantage-normalised updates and larger batch size. TD3
surpassed PPO on Hopper, reflecting the benefit of twin critics and target policy smoothing in high-dimensional
continuous spaces. Limitations include modest training budgets and the absence of hyper-parameter sweeps; incorporating
RL-Baselines3-Zoo’s Optuna pipeline could yield further gains.

## References

1. Gymnasium documentation, Classic Control and MuJoCo pages【turn0search3】【turn0search8】
2. MuJoCo installation guides for Python wheels on macOS【turn0search0】【turn0search5】
3. Stable-Baselines3 repository and release notes【turn0search6】【turn0search1】
4. RL-Baselines3-Zoo framework and pretrained agents archive【turn0search2】【turn0search7】
5. IMRAD best-practice README examples in ML projects【turn0search4】  
