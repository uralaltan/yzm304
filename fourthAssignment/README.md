# Fourth Assignment â€“ Deep RL Benchmarks

## Introduction

Reinforcement learning Discussion

The benchmark results reveal distinct strengths among the three reinforcement learning algorithms tested on CarRacing-v3:

**SAC emerged as the top performer** (6.94 Â± 0.47), demonstrating the effectiveness of maximum entropy reinforcement learning for continuous control tasks. The automatic entropy coefficient tuning and sample-efficient off-policy learning contributed to its superior mean performance while maintaining reasonable consistency.

**TD3 achieved competitive performance** (6.86 Â± 0.60) through its twin critic architecture and delayed policy updates, which address the overestimation bias common in actor-critic methods. However, it exhibited the highest variance, suggesting sensitivity to initial conditions or hyperparameter settings.

**PPO provided the most consistent results** (6.77 Â± 0.37) despite having the lowest mean reward. Its on-policy nature and clipped objective function ensure stable learning, making it highly reliable for applications where consistent performance is prioritized over peak performance.

### Algorithm Selection Guidelines

- **Choose SAC** for maximum performance in continuous control tasks requiring exploration
- **Choose PPO** when consistency and stability are more important than peak performance
- **Choose TD3** when deterministic policies are preferred and you can afford hyperparameter tuning

### Limitations and Future Work

The current evaluation uses a limited training budget (1,000 timesteps) for rapid comparison. Extended training sessions would likely reveal more pronounced performance differences and provide insights into long-term learning dynamics. Additionally, hyperparameter optimization could improve all algorithms' performance significantly.

For comprehensive analysis and detailed algorithm explanations, see the [detailed blog post](blog/README.md).esearch relies on reproducible benchmarks across various environments to evaluate and compare different algorithms. This assignment implements and evaluates three state-of-the-art reinforcement learning algorithmsâ€”Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), and Twin Delayed Deep Deterministic Policy Gradient (TD3)â€”on the challenging CarRacing-v3 continuous control environment. Using Stable-Baselines3 (SB3) implementations, we provide a comprehensive comparison of these algorithms' performance on a vision-based driving task.

## Method

| Component            | File                                  | Purpose                                                        |
|----------------------|---------------------------------------|----------------------------------------------------------------|
| PPO Implementation   | `car_racing_ppo.py`                   | PPO training on CarRacing-v3 with CNN policy                  |
| SAC Implementation   | `car_racing_sac.py`                   | SAC training on CarRacing-v3 with CNN policy                  |
| TD3 Implementation   | `car_racing_td3.py`                   | TD3 training on CarRacing-v3 with CNN policy                  |
| Evaluation script    | `evaluate.py`                         | Runs 20 rollouts per model and prints performance metrics     |
| Visualization tool   | `visualize.py`                        | Interactive visualization of trained agents                    |
| Saved models         | `models/`                             | Contains trained model weights and normalization statistics    |
| Analysis blog        | `blog/README.md`                      | Comprehensive analysis and algorithm comparison                |

### Environment Setup

**CarRacing-v3 Environment:**
- **Observation Space**: RGB images (96Ã—96Ã—3 pixels)
- **Action Space**: Continuous [steering, gas, brake]
- **Challenge**: Vision-based continuous control with spatial reasoning

**Preprocessing Pipeline:**
- Frame stacking (4 consecutive frames)
- Image transposition for CNN compatibility
- Observation normalization with VecNormalize
- Vectorized environment wrapper

### Algorithm Configurations

**PPO (On-policy):**
- n_steps: 256, batch_size: 64, n_epochs: 4
- learning_rate: 2.5e-4, clip_range: 0.1
- GAE lambda: 0.95, gamma: 0.99

**SAC (Off-policy with entropy regularization):**
- buffer_size: 10,000, batch_size: 256
- learning_rate: 3e-4, tau: 0.005
- Automatic entropy coefficient tuning

**TD3 (Off-policy with twin critics):**
- buffer_size: 10,000, batch_size: 256
- learning_rate: 3e-4, policy_delay: 2
- Target policy noise: 0.2, noise clip: 0.5

**Hardware & Software:**
* MacBook Pro M4, 16 GB RAM, macOS 15
* Python 3.12, Stable-Baselines3 2.4.0
* Gymnasium 1.5.0, OpenCV for image processing

**Training Budget:**
* All algorithms: 1,000 timesteps (for quick comparison)
* Evaluation: 20 episodes per algorithm

## Results

| Algorithm | Mean Reward | Standard Deviation | Ranking | Key Characteristics |
|-----------|-------------|-------------------|---------|-------------------|
| **SAC**   | **6.94**    | 0.47              | ðŸ¥‡ 1st   | Best performance, automatic exploration |
| **TD3**   | 6.86        | 0.60              | ðŸ¥ˆ 2nd   | Strong performance, higher variance |
| **PPO**   | 6.77        | **0.37**          | ðŸ¥‰ 3rd   | Most consistent, lowest variance |

### Performance Analysis

**Relative Performance Improvements:**
- SAC vs PPO: +0.17 points (+2.5% improvement)
- TD3 vs PPO: +0.09 points (+1.3% improvement)  
- SAC vs TD3: +0.08 points (+1.2% advantage)

**Consistency Ranking:**
1. PPO: Ïƒ = 0.37 (most reliable)
2. SAC: Ïƒ = 0.47 (moderate variance)
3. TD3: Ïƒ = 0.60 (highest variance)

## Discussion

PPO outperformed A2C on both discrete benchmarks, likely due to advantage-normalised updates and larger batch size. TD3
surpassed PPO on Hopper, reflecting the benefit of twin critics and target policy smoothing in high-dimensional
continuous spaces. Limitations include modest training budgets and the absence of hyper-parameter sweeps; incorporating
RL-Baselines3-Zooâ€™s Optuna pipeline could yield further gains.

## Usage

### Training Models
```bash
# Train individual algorithms
python car_racing_ppo.py
python car_racing_sac.py  
python car_racing_td3.py
```

### Running Benchmarks
```bash
# Evaluate all trained models
python evaluate.py
```

### Visualizing Results
```bash
# Visualize specific algorithm
python visualize.py --algo sac --model models/sac_car_racing.zip

# Available options: ppo, sac, td3
python visualize.py --algo ppo --model models/ppo_car_racing.zip --headless
```

## References

1. Schulman, J., et al. "Proximal Policy Optimization Algorithms." arXiv preprint arXiv:1707.06347 (2017)
2. Haarnoja, T., et al. "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor." ICML (2018)
3. Fujimoto, S., et al. "Addressing Function Approximation Error in Actor-Critic Methods." ICML (2018)
4. Stable-Baselines3 Documentation: https://stable-baselines3.readthedocs.io/
5. Gymnasium CarRacing-v3: https://gymnasium.farama.org/environments/box2d/car_racing/  
